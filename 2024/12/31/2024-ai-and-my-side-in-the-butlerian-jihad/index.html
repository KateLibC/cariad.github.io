<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>2024, AI, and my side in the Butlerian Jihad</title>
  <meta name="description" content="Depiction of the Butlerian Jihad from the books Earlier this year, my friend asked me to come over to her place to watch Dune Part One so she could the next day drag me to see Part Two in cinema. After that weekend, I gained a new set of worlds to explore: a universe once tainted by artificial intelligence but no longer thanks to a war with “thinking machines”. This wasn’t my first experience with Dune, as I had at one point in my past watched the David Lynch rendition, which came out the year I was born. However, the Denis Villeneuve version captured my attention due to its non-goofy approach to the story, and as a consequence I now find myself reading through the books. One plot device of the (known) universe in Frank Herbert’s Dune is the lack of anything resembling a computer as we know it. Approximately ten thousand years before protagonist Paul Atreides appearance in the story, the established royal houses were engaged in a war — dubbed the “Butlerian Jihad” — with thinking machines where humanity was victorious. Thufir Hawat, a mentat as depicted in Dune (2021), played by Stephen McKinley This victory led to anything resembling a thinking machine considered as forbidden technology and instead led to humanity being dependent on but not limited to mentats (human computers basically), analogue technology, and genetic modification including eugenics (unfortunate). In the real world of 2024, we don’t have to worry about computers having sentience and we never will in 2025, 2026, 2032, or 2038 — check in with me in 2039 for an update. The suggestion that it will achieve artificial general intelligence (AGI) any time soon is absolute nonsense — the power requirements that AI-championing OpenAI is desiring is the equivalent to the output of some countries. Just to map one cubic millimetre of the human brain required 1.4 petabytes of storage, meaning that to do its entirety, based on its average size of 2,174,340 mm², you’d need 3,044,076 petabytes of storage. If you convert that to zettabytes, you’d end up with about 3, which about 2% of what is suggested as the entire Internet’s total storage capability. This is for one human brain. The entire Internet infrastructure is suggested to consume about 800 TWh of electricity, meaning that to simulate this one human brain, you’d need 16 TWh just for storage — this is enough to power all of Cuba just to put it all into context and we’re not even considering all of the other technicals we’d need to examine to pull it off. At 640 grams each, you’d have 64,000,000 KG of these bad boys floating around for just one brain, putting them all at around 20% the total weight of the Empire State Building We cannot map the entire brain without having to have nearly 100 million of these hard drives spinning at any given time (imagine the failure rate you’d have to contend with) and yet we’re expected to believe arrogant pricks like Sam Altman have any clue about what its usefulness is? Even if we play into the fool’s statement that we only use 10% of our brains (this is untrue), we’d still need just over 1 TWh for storage for a human brain. Why are we suggesting that we go and speed up climate change in favour of a machine that relies on a garbage in garbage out philosophy? Recently, I became aware that I was put on someone’s list as “someone who doesn’t get AI/NFTs/crypto” — here’s the thing: I do understand it, but what I don’t get is why do these types of people who champion these fads never take a step back to understand them? I have a decade and a half experience with computer security and have seen so much change, but what hasn’t changed is the creature in front of the display. Humans are complex creatures; we don’t understand why we have consciousness and yet we have clowns in the world who suggest we can replicate some or all of it? Quantum computers probably won’t help either here — if they even ever work. The idea of classical computers with their binary states achieving the intelligence of a human brain is short-sighted — the idiocy I already am making an argument for here. A thousand lines of code is more than enough to power a large language model, which forms the basis for the AI software we see commonly today, but to suggest that is enough in contrast to the billions of years of evolution that led to our species being here today is pure hubris and will lead to us hoisting ourselves by our own petard. Were the Butlerian Jihad were to start today, I know which side I’d find myself on. Stop boiling the damn ocean to make soulless art, worthless and insecure code, and education materials of which are lacking in facts. You have a brain; use it, exercise it, and find what you’re actually capable of.">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://cariad.keigher.ca/2024/12/31/2024-ai-and-my-side-in-the-butlerian-jihad/">
  
  
  <link rel="alternate" type="application/rss+xml" title="cariad.keigher.ca" href="https://cariad.keigher.ca/feed.xml">

  

  
  <meta property="og:title" content="2024, AI, and my side in the Butlerian Jihad">
  <meta property="og:site_name" content="cariad.keigher.ca">
  <meta property="og:url" content="https://cariad.keigher.ca/2024/12/31/2024-ai-and-my-side-in-the-butlerian-jihad/">
  <meta property="og:description" content="Depiction of the Butlerian Jihad from the books Earlier this year, my friend asked me to come over to her place to watch Dune Part One so she could the next day drag me to see Part Two in cinema. After that weekend, I gained a new set of worlds to explore: a universe once tainted by artificial intelligence but no longer thanks to a war with “thinking machines”. This wasn’t my first experience with Dune, as I had at one point in my past watched the David Lynch rendition, which came out the year I was born. However, the Denis Villeneuve version captured my attention due to its non-goofy approach to the story, and as a consequence I now find myself reading through the books. One plot device of the (known) universe in Frank Herbert’s Dune is the lack of anything resembling a computer as we know it. Approximately ten thousand years before protagonist Paul Atreides appearance in the story, the established royal houses were engaged in a war — dubbed the “Butlerian Jihad” — with thinking machines where humanity was victorious. Thufir Hawat, a mentat as depicted in Dune (2021), played by Stephen McKinley This victory led to anything resembling a thinking machine considered as forbidden technology and instead led to humanity being dependent on but not limited to mentats (human computers basically), analogue technology, and genetic modification including eugenics (unfortunate). In the real world of 2024, we don’t have to worry about computers having sentience and we never will in 2025, 2026, 2032, or 2038 — check in with me in 2039 for an update. The suggestion that it will achieve artificial general intelligence (AGI) any time soon is absolute nonsense — the power requirements that AI-championing OpenAI is desiring is the equivalent to the output of some countries. Just to map one cubic millimetre of the human brain required 1.4 petabytes of storage, meaning that to do its entirety, based on its average size of 2,174,340 mm², you’d need 3,044,076 petabytes of storage. If you convert that to zettabytes, you’d end up with about 3, which about 2% of what is suggested as the entire Internet’s total storage capability. This is for one human brain. The entire Internet infrastructure is suggested to consume about 800 TWh of electricity, meaning that to simulate this one human brain, you’d need 16 TWh just for storage — this is enough to power all of Cuba just to put it all into context and we’re not even considering all of the other technicals we’d need to examine to pull it off. At 640 grams each, you’d have 64,000,000 KG of these bad boys floating around for just one brain, putting them all at around 20% the total weight of the Empire State Building We cannot map the entire brain without having to have nearly 100 million of these hard drives spinning at any given time (imagine the failure rate you’d have to contend with) and yet we’re expected to believe arrogant pricks like Sam Altman have any clue about what its usefulness is? Even if we play into the fool’s statement that we only use 10% of our brains (this is untrue), we’d still need just over 1 TWh for storage for a human brain. Why are we suggesting that we go and speed up climate change in favour of a machine that relies on a garbage in garbage out philosophy? Recently, I became aware that I was put on someone’s list as “someone who doesn’t get AI/NFTs/crypto” — here’s the thing: I do understand it, but what I don’t get is why do these types of people who champion these fads never take a step back to understand them? I have a decade and a half experience with computer security and have seen so much change, but what hasn’t changed is the creature in front of the display. Humans are complex creatures; we don’t understand why we have consciousness and yet we have clowns in the world who suggest we can replicate some or all of it? Quantum computers probably won’t help either here — if they even ever work. The idea of classical computers with their binary states achieving the intelligence of a human brain is short-sighted — the idiocy I already am making an argument for here. A thousand lines of code is more than enough to power a large language model, which forms the basis for the AI software we see commonly today, but to suggest that is enough in contrast to the billions of years of evolution that led to our species being here today is pure hubris and will lead to us hoisting ourselves by our own petard. Were the Butlerian Jihad were to start today, I know which side I’d find myself on. Stop boiling the damn ocean to make soulless art, worthless and insecure code, and education materials of which are lacking in facts. You have a brain; use it, exercise it, and find what you’re actually capable of.">
  
  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="2024, AI, and my side in the Butlerian Jihad">
  <meta name="twitter:description" content="Depiction of the Butlerian Jihad from the books Earlier this year, my friend asked me to come over to her place to watch Dune Part One so she could the next day drag me to see Part Two in cinema. A...">
  
  

  <link rel="dns-prefetch" href="https://fonts.gstatic.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Bitter:ital,wght@0,400;0,700;1,400&amp;display=swap" rel="stylesheet">

  

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">cariad.keigher.ca</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/archives/">Archives</a>
      
        
        <a class="page-link" href="https://hachyderm.io/@KateLibC">Mastodon</a>
      
        
        <a class="page-link" href="https://bsky.app/profile/cariad.bsky.social">Bluesky</a>
      
        
        <a class="page-link" href="https://github.com/katelibc">GitHub</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
      <h1 class="post-title" itemprop="name headline">2024, AI, and my side in the Butlerian Jihad</h1>
    
    <p class="post-meta"><time datetime="2024-12-31T00:00:00+00:00" itemprop="datePublished">Dec 31, 2024</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <hr />

<p><img src="/img/0vChTdqoDivbqEG5__1" alt="" /></p>

<p>Depiction of the Butlerian Jihad from the books</p>

<p>Earlier this year, my friend asked me to come over to her place to watch Dune Part One so she could the next day drag me to see Part Two in cinema. After that weekend, I gained a new set of worlds to explore: a universe once tainted by artificial intelligence but no longer thanks to a war with “<a href="https://breezewiki.com/dune/wiki/Thinking_Machines">thinking machines</a>”.</p>

<p>This wasn’t my first experience with Dune, as I had at one point in my past watched the <a href="https://en.wikipedia.org/wiki/Dune_%281984_film%29">David Lynch rendition</a>, which came out the year I was born. However, the Denis Villeneuve version captured my attention due to its non-goofy approach to the story, and as a consequence I now find myself reading through the books.</p>

<p>One plot device of the (known) universe in Frank Herbert’s Dune is the lack of anything resembling a computer as we know it. Approximately ten thousand years before protagonist <a href="https://en.wikipedia.org/wiki/Paul_Atreides">Paul Atreides</a> appearance in the story, the established royal houses were engaged in a war — dubbed the “<a href="https://breezewiki.com/dune/wiki/Butlerian_Jihad">Butlerian Jihad</a>” — with thinking machines where humanity was victorious.</p>

<p><img src="/img/0Qbo4gkyFYhTL6MqV_1" alt="" /></p>

<p>Thufir Hawat, a mentat as depicted in Dune (2021), played by Stephen McKinley</p>

<p>This victory led to anything resembling a thinking machine considered as forbidden technology and instead led to humanity being dependent on but not limited to <a href="https://breezewiki.com/dune/wiki/Mentat">mentats</a> (human computers basically), analogue technology, and genetic modification including eugenics (unfortunate).</p>

<p>In the real world of 2024, we don’t have to worry about computers having sentience and we never will in 2025, 2026, 2032, or 2038 — check in with me in 2039 for an update. <a href="https://gizmodo.com/openai-claims-its-new-model-reached-human-level-on-a-test-for-general-intelligence-what-does-that-mean-2000543834">The suggestion that it will achieve artificial general intelligence</a> (AGI) any time soon is absolute nonsense — the power requirements that AI-championing OpenAI is desiring <a href="https://finance.yahoo.com/news/openai-reportedly-wants-build-5-115944436.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAKGpMqYtIi0gew2IJPTVRrBWdZuBooCyleaDSdj3BwoDaivqVu2AxLMjfVv-8v9nG8MzT-i9dp8U77u8pJpRpHyLFXdHFX88H3vSQRIkoHjxzLIvReCvgncpICZnvN9EzSiaDlwJsIFLp-TdP1HWEUaMUGR7BIql1DTJeTQS8tdQ">is the equivalent to the output of some countries</a>.</p>

<p>Just <a href="https://www.scientificamerican.com/article/a-cubic-millimeter-of-a-human-brain-has-been-mapped-in-spectacular-detail/">to map one cubic millimetre of the human brain required 1.4 petabytes of storage</a>, meaning that to do its entirety, based on its average <a href="https://www.verywellmind.com/how-big-is-the-brain-2794888">size</a> of 2,174,340 mm², you’d need 3,044,076 petabytes of storage. If you convert that to zettabytes, you’d end up with about 3, which <a href="https://rivery.io/blog/big-data-statistics-how-much-data-is-there-in-the-world/">about 2% of what is suggested as the entire Internet’s total storage capability</a>.</p>

<p>This is for one human brain. The entire Internet infrastructure is <a href="https://thundersaidenergy.com/downloads/internet-energy-consumpion-data-models-forecasts/#:~:text=Our%20best%20estimate%20is%20that,2.5%25%20of%20all%20global%20electricity.">suggested to consume about 800 TWh of electricity</a>, meaning that to simulate this one human brain, you’d need 16 TWh just for storage — this is enough to <a href="https://en.wikipedia.org/wiki/List_of_countries_by_electricity_production">power all of Cuba</a> just to put it all into context and we’re not even considering all of the other technicals we’d need to examine to pull it off.</p>

<p><img src="/img/1gaZdChB_XbH-9wsh_2lYSA_1.png" alt="" /></p>

<p>At 640 grams each, you’d have 64,000,000 KG of these bad boys floating around for just one brain, putting them all at around 20% the total weight of the Empire State Building</p>

<p>We cannot map the entire brain without having to have nearly <a href="https://www.westerndigital.com/products/internal-drives/data-center-drives/ultrastar-dc-hc690-hdd?sku=WSH723220AL4201">100 million of these hard drives</a> spinning at any given time (imagine the failure rate you’d have to contend with) and yet we’re expected to believe arrogant pricks like <a href="https://en.wikipedia.org/wiki/Sam_Altman">Sam Altman</a> have any clue about what its usefulness is?</p>

<p>Even if we play into the fool’s statement that we only use 10% of our brains (this is untrue), we’d still need just over 1 TWh for storage for a human brain. Why are we suggesting that we go and speed up climate change in favour of a machine that relies on a garbage in garbage out philosophy?</p>

<p>Recently, I became aware that I was put on someone’s list as “someone who doesn’t get AI/NFTs/crypto” — here’s the thing: I do understand it, but what I don’t get is why do these types of people who champion these fads never take a step back to understand them?</p>

<p>I have a decade and a half experience with computer security and have seen so much change, but what hasn’t changed is the creature in front of the display. Humans are complex creatures; <a href="https://en.wikipedia.org/wiki/Consciousness#Scientific_study">we don’t understand why we have consciousness</a> and yet we have clowns in the world who suggest we can replicate some or all of it?</p>

<p><img src="/img/0Sl7c2Aacvr2G8oyM_1" alt="" /></p>

<p>Quantum computers probably won’t help either here — if they even ever work.</p>

<p>The idea of classical computers with their binary states achieving the intelligence of a human brain is short-sighted — the idiocy I already am making an argument for here.</p>

<p>A thousand lines of code is more than enough to power a <a href="https://en.wikipedia.org/wiki/Large_language_model">large language model</a>, which forms the basis for the AI software we see commonly today, but to suggest that is enough in contrast to the billions of years of evolution that led to our species being here today is pure hubris and will lead to us hoisting ourselves by our own petard.</p>

<p>Were the Butlerian Jihad were to start today, I know which side I’d find myself on. Stop boiling the damn ocean to make soulless art, worthless and insecure code, and education materials of which are lacking in facts.</p>

<p>You have a brain; use it, exercise it, and find what you’re actually capable of.</p>

  </div>

  

</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy; Cariad Heather Keigher - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="https://cariad.keigher.ca/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>
